<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>PyTorch 随机 Mask 的技巧</title>
    <link href="/2023/04/27/pytorch-random-mask/"/>
    <url>/2023/04/27/pytorch-random-mask/</url>
    
    <content type="html"><![CDATA[<p>本文记录一下用 PyTorch 随机 Mask 的技巧。</p><p>这里假设数值低于 2 的 token 都是特殊 token，不做处理。</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torchmask_token_id <span class="token operator">=</span> <span class="token number">4</span><span class="token keyword">def</span> <span class="token function">mlm</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token comment"># 50% 的概率随机 mask</span>    <span class="token comment"># 忽略掉数值低于 2 的特殊 token</span>    mask_arr <span class="token operator">=</span> <span class="token punctuation">(</span>rand <span class="token operator">&lt;</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>tensor <span class="token operator">></span> <span class="token number">2</span><span class="token punctuation">)</span>     <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        selection <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>mask_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>        tensor<span class="token punctuation">[</span>i<span class="token punctuation">,</span> selection<span class="token punctuation">]</span> <span class="token operator">=</span> mask_token_id    <span class="token keyword">return</span> tensor<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>测试：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1652</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">3252</span><span class="token punctuation">,</span> <span class="token number">1234</span><span class="token punctuation">,</span> <span class="token number">634</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">223</span><span class="token punctuation">,</span> <span class="token number">1530</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">4134</span><span class="token punctuation">,</span> <span class="token number">832</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span>labels <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>masks <span class="token operator">=</span> mlm<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"labels: \n"</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"masks: \n"</span><span class="token punctuation">,</span> masks<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>输出结果（因为是随机 mask，每个人的输出会有所不同）:</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">labels:tensor([    [0, 1652,  233, 3252, 1234,  634,    1,    1,    1,    1],    [0,  223, 1530,  232, 4134,  832,   20,    1,    1,    1]])masks: tensor([    [0,    4,    4,    4, 1234,  634,    1,    1,    1,    1],    [0,    4,    4,  232,    4,  832,    4,    1,    1,    1]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>凡是出现数值 4 的，都是被 mask 掉的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【学习笔记】强化学习基本概念</title>
    <link href="/2023/04/19/drl-study-notes-basic-concept/"/>
    <url>/2023/04/19/drl-study-notes-basic-concept/</url>
    
    <content type="html"><![CDATA[<p>为了加强理解，我们以《超级玛丽》游戏为例，让 AI 使用强化学习，操作马里奥走动和跳跃，完成通关任务。</p><p><a href="https://youtu.be/vmkRMvhCW5c" title="深度强化学习(1/5)：基本概念 Deep Reinforcement Learning (1/5)"><img src="https://res.cloudinary.com/marcomontalbano/image/upload/v1681899633/video_to_markdown/images/youtube--vmkRMvhCW5c-c05b58ac6eb4c4700831b2b3070cd403.jpg" alt="深度强化学习(1/5)：基本概念 Deep Reinforcement Learning (1/5)" /></a></p><h2 id="术语">术语</h2><h3 id="agent">Agent</h3><p>用强化学习驱动的作用对象，也就是下面 Action 的作用对象。在《超级玛丽》游戏中，这个 Agent 就是 AI 控制的马里奥。</p><h3 id="state">State</h3><p>当前系统状态，用 <span class="math inline">\(s\)</span> 表示。</p><h3 id="action">Action</h3><p>作用对象执行的操作或动作，用 <span class="math inline">\(a\)</span> 表示。例如，在《超级玛丽》游戏中，所有的操作包括往左走 (left)、往右走 (right)、往上走 (up)。那么，</p><p><span class="math display">\[a \in \{left, right, up\}\]</span></p><h3 id="policy">Policy</h3><p>策略函数，用 <span class="math inline">\(\pi\)</span> 表示。策略函数是根据当前观测到的状态，控制 Agent 如何从动作库（Action）中选择一种动作来执行。策略函数是一个条件概率密度函数，即 <span class="math inline">\(\pi :(s, a) \mapsto [0, 1]\)</span></p><p><span class="math display">\[\pi (a | s) = \mathbb{P}(A = a | S = s)\]</span></p><p>例如,</p><ul><li><p><span class="math inline">\(\pi(left | s) = 0.5\)</span></p></li><li><p><span class="math inline">\(\pi(right | s) = 0.3\)</span></p></li><li><p><span class="math inline">\(\pi(up | s) = 0.2\)</span></p></li></ul><p><strong>强化学习就是学习出这个策略函数</strong>。基于策略函数，控制 Agent 每一步选择出一个最优动作，完成目标。</p><h3 id="reward">Reward</h3><p>奖励，用 <span class="math inline">\(R\)</span> 表示。Agent 每做一次动作，系统就会给与一定的奖励。<strong>奖励定义的好坏非常影响强化学习的结果。</strong>例如，在《超级玛丽》游戏中，</p><ul><li><p>搜集到一个金币：<span class="math inline">\(R = +1\)</span></p></li><li><p>赢得比赛：<span class="math inline">\(R = +10000\)</span></p></li><li><p>输掉比赛：<span class="math inline">\(R = -10000\)</span></p></li><li><p>什么都没发生：<span class="math inline">\(R = 0\)</span></p></li></ul><p>强化学习的目标，是<strong>获得的奖励总和要尽量高</strong>。</p><h3 id="state-transition">State Transition</h3><p>状态转移。在执行了一个动作后，系统从一个状态变成另一个状态的过程，就是状态转移。用下面的图表示： <pre><code class="mermaid" >graph LRA[old state]--action--->B[new state]</code></pre></p><p>状态转移可以是随机的。这种随机性来源于环境 (Environment)。状态转移满足如下的公式：</p><p><span class="math display">\[p(s&#39;|s, a) = \mathbb{P}(S&#39; = s&#39; | S = s, A = a)\]</span></p><h4 id="强化学习的随机性来源">强化学习的随机性来源</h4><ul><li><p>Action 有随机性</p></li><li><p>State Transition 有随机性</p></li></ul><h4 id="执行流程">执行流程</h4><pre><code class="mermaid" >graph TDS1["s(1)"]-->A1{"a(1)"}A1-->S2["s(2)"]A1-.->R1("r(1)")S2-->A2{"a(2)"}A2-->S3["s(3)"]A2-.->R2("r(2)")S3--"……"--->ST["s(T)"]ST-->AT{"a(T)"}AT-->ST1["s(T+1)"]AT-.->RT("r(T)")</code></pre><p>形成 <code>(state, action, reward)</code> 的轨迹 (trajectory): <span class="math display">\[(s_1,a_1,r_1), (s_2,a_2,r_2), \dots , (s_T, a_T, r_T)\]</span></p><h3 id="return">Return</h3><p>回报，未来的累计奖励 (Cumulative future reward)。<span class="math inline">\(t\)</span> 时刻的未来累计回报用 <span class="math inline">\(U_t\)</span> 表示，有如下公式：</p><p><span class="math display">\[U_t = R_t + R_{t+1} + R_{t+2} + R_{t+3} + \dots\]</span></p><p>从经验来看，未来的奖励 (reward) 有时不如当下的奖励来得有价值。 例如，当下的 100 元比一年后的 100 元更有价值。 为了体现这一点，我们可以给未来的每一项奖励乘上一个取值范围是 <code>[0, 1]</code> 的因子，变成</p><p><span class="math display">\[U_t = R_t + \gamma^{1}R_{t+1} + \gamma^{2}R_{t+2} + \gamma^{3}R_{t+3} + \dots\]</span></p><p>形成折扣回报 (<strong>Discounted</strong> return，即 Cumulative <strong>discounted</strong> future reward)。其中 <span class="math inline">\(\gamma\)</span> 是介于 0 到 1 之间的折扣率 (discount rate)。 折扣率是一项超参数 (Hyper-parameter)，可以在训练时被调优。</p><p><span class="math inline">\(t\)</span> 时刻的回报是随机的，这是因为回报依赖于未来的奖励；而未来的奖励则依赖于未来的动作和状态。 后两者都具有随机性，这也使得回报必然具有随机性。用数学公式再来整理一遍：</p><p>由于：</p><ol type="1"><li>动作具有随机性: <span class="math display">\[\mathbb{P}(A = a | S = s) = \pi (a | s)\]</span></li><li>状态转移具有随机性: <span class="math display">\[\mathbb{P}(S&#39; = s&#39; | S = s, A = a) = p(s&#39;|s, a)\]</span></li></ol><p>对于任意的 <span class="math inline">\(i \ge t\)</span>，奖励 <span class="math inline">\(R_i\)</span> 依赖于 <span class="math inline">\(S_i\)</span> 和 <span class="math inline">\(A_i\)</span>。因此，给定 <span class="math inline">\(S_t\)</span>, 回报 <span class="math inline">\(U_t\)</span> 应依赖于随机变量：<span class="math inline">\(A_t, A_{t+1}, A_{t+2}, \dots\)</span> 和 <span class="math inline">\(S_t, S_{t+1}, S_{t+2}, \dots\)</span></p><h3 id="value-functions">Value Functions</h3><h4 id="action-value-function">Action-Value Function</h4><p>动作价值函数，是在某一时刻 <span class="math inline">\(t\)</span>，对给定的策略函数 <span class="math inline">\(\pi\)</span>，输出某个动作 <span class="math inline">\(a_t\)</span>，得到未来回报期望的函数。用 <span class="math inline">\(Q\)</span> 表示。即</p><p><span class="math display">\[Q_{\pi}(s_t, a_t) = \mathbb{E}[U_t|S_t=s_t, A_t=a_t]\]</span></p><p><span class="math inline">\(U_t\)</span> 是<span class="math inline">\(a_t, a_{t+1}, a_{t+2}, \dots\)</span> 和 <span class="math inline">\(s_t, s_{t+1}, s_{t+2}, \dots\)</span> 的函数。</p><p><span class="math display">\[U_t = U(a_t, a_{t+1}, a_{t+2}, \dots, s_t, s_{t+1}, s_{t+2}, \dots)\]</span></p><p>计算期望时，所有的<span class="math inline">\(a_{t+1}, a_{t+2}, \dots\)</span> 和 <span class="math inline">\(s_{t+1}, s_{t+2}, \dots\)</span> 都应被积分（或累加）掉，最后只保留 <span class="math inline">\(a_t\)</span> 和 <span class="math inline">\(s_t\)</span>。</p><p><span class="math inline">\(t\)</span> 时刻对不同的策略函数分别求期望，找出期望值最大的那个动作价值函数，即可得到<strong>最优动作价值函数 (Optimal action-value function)</strong>。 <span class="math display">\[Q^*(s_t, a_t) = \max_{\pi}Q_{\pi}(s_t, a_t)\]</span></p><p><span class="math inline">\(Q^*\)</span> 与 <span class="math inline">\(\pi\)</span> 无关。</p><h4 id="state-value-function">State-Value Function</h4><p>对所有的动作 <span class="math inline">\(A\)</span>，计算 <span class="math inline">\(Q\)</span> 的期望，可得到状态价值函数（State-value function)。用下面公式表示：</p><p><span class="math display">\[V_{\pi}(s_t) = \mathbb{E}_A[Q_{\pi}(s_t, A)]\]</span></p><p>因为 <span class="math inline">\(A\)</span> 服从策略函数定义的概率分布，即 <span class="math inline">\(A \sim \pi(\cdot|s_t)\)</span>，因此上式也可以写成</p><p>（动作是离散的情况） <span class="math display">\[V_{\pi}(s_t) = \mathbb{E}_A[Q_{\pi}(s_t, A)] = \sum_a \pi(a|s_t)\cdot Q_{\pi}(s_t, a)\]</span> （动作是连续的情况） <span class="math display">\[V_{\pi}(s_t) = \mathbb{E}_A[Q_{\pi}(s_t, A)] = \int_a \pi(a|s_t)\cdot Q_{\pi}(s_t, a) da\]</span></p><h4 id="两种价值函数的作用">两种价值函数的作用</h4><ul><li><p>动作价值函数评估的是，对于给定的策略函数 <span class="math inline">\(\pi\)</span>，Agent 处在状态 <span class="math inline">\(s\)</span> 时，选择动作 <span class="math inline">\(a\)</span> 的好坏。</p></li><li><p>状态价值函数评估的是，对于给定的策略函数 <span class="math inline">\(\pi\)</span>，Agent 处在状态 <span class="math inline">\(s\)</span> 的好坏。</p></li></ul><p>如果要评估策略函数本身的好坏，可以对所有状态 <span class="math inline">\(S\)</span> 求状态价值函数的期望，即 <span class="math display">\[\mathbb{E}_S[V_{\pi}(S)]\]</span></p><h2 id="ai-如何控制-agent">AI 如何控制 Agent</h2><ul><li><p>假设我们有一个好的策略函数 <span class="math inline">\(\pi(a|s)\)</span>，那么我们只需要基于当前时刻的状态 <span class="math inline">\(s_t\)</span>，随机采样一个动作即可: <span class="math display">\[a_t \sim \pi(\cdot|s_t)\]</span></p></li><li><p>假设我们知道最优动作价值函数 <span class="math inline">\(Q^*(s, a)\)</span>，那么我们只需要基于当前时刻的状态 <span class="math inline">\(s_t\)</span>，选择一个可以最大化动作价值函数的动作即可：</p></li></ul><p><span class="math display">\[a_t = \arg\max_a Q^*(s_t, a)\]</span></p><p>由此可见，强化学习的目标既可以是<strong>策略函数</strong> <span class="math inline">\(\pi(a|s)\)</span>，也可以是<strong>最优动作价值函数</strong> <span class="math inline">\(Q^*(s, a)\)</span>。</p><h2 id="参考">参考</h2><ul><li><a href="https://github.com/wangshusen/DRL">Deep Reinforcement Learning</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>DRL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Agent</tag>
      
      <tag>Environment</tag>
      
      <tag>State</tag>
      
      <tag>Action</tag>
      
      <tag>Reward</tag>
      
      <tag>Policy</tag>
      
      <tag>StateTransition</tag>
      
      <tag>Return</tag>
      
      <tag>ValueFunctions</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在生产环境中使用 Docker 部署 Flask 服务</title>
    <link href="/2023/04/18/flask-uwsgi-docker/"/>
    <url>/2023/04/18/flask-uwsgi-docker/</url>
    
    <content type="html"><![CDATA[<h2 id="用-anaconda-开启一个新的环境">用 Anaconda 开启一个新的环境</h2><p>创建一个新的 conda 环境，并取名为 <code>flask</code></p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda create <span class="token parameter variable">--name</span> flask <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>激活 <code>flask</code> 环境</p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda activate flask<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><h2 id="开启一个简单的-flask-服务">开启一个简单的 Flask 服务</h2><p>用 pip 安装 Flask</p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> flask<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># main.py</span><span class="token keyword">from</span> flask <span class="token keyword">import</span> Flask<span class="token punctuation">,</span> request<span class="token punctuation">,</span> jsonifyapp <span class="token operator">=</span> Flask<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>app<span class="token punctuation">.</span>config<span class="token punctuation">[</span><span class="token string">'JSON_AS_ASCII'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token decorator annotation punctuation">@app<span class="token punctuation">.</span>route</span><span class="token punctuation">(</span><span class="token string">"/hello"</span><span class="token punctuation">,</span> methods<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"GET"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">hello_world</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> request<span class="token punctuation">.</span>args<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> jsonify<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>        <span class="token string">"err_code"</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>        <span class="token string">"ret"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f"Hello, </span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    app<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>运行程序，得到如下输出结果：</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none"> * Serving Flask app &#39;main&#39; * Debug mode: offWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on http:&#x2F;&#x2F;127.0.0.1:5000Press CTRL+C to quit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>然后浏览器打开：</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">http:&#x2F;&#x2F;127.0.0.1:5000&#x2F;hello?name&#x3D;AI探险家<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>就会从浏览器中得到如下结果：</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">&#123;&quot;err_code&quot;:0,&quot;ret&quot;:&quot;Hello, AI探险家&quot;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>从前面开启服务的输出结果可以看出，我们的 Flask 应用目前是一个开发服务。 要正式部署的话，建议使用 WSGI。那我们就接受建议，使用 <code>uWSGI</code> 搭配 <code>nginx</code> 来部署服务搭配。</p><h2 id="安装-uwsgi">安装 uWSGI</h2><p>首先安装 uWSGI 相关的依赖环境。下面是 Ubuntu 系统下的安装命令</p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">apt-get</span> <span class="token function">install</span> build-essential python-dev<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>其他操作系统可以参考官方安装教程：<a href="https://uwsgi-docs.readthedocs.io/en/latest/Install.html">Installing uWSGI</a></p><p>接下来是安装 <code>uwsgi</code>。理论上可以直接用 <code>pip</code> 安装。</p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> uwsgi<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>奇怪的是，我这边在 Anaconda 环境中用 <code>pip</code> 安装 <code>uwsgi</code> 会报错。用 <code>conda</code> 命令来安装则是正常的：</p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda <span class="token function">install</span> <span class="token parameter variable">-c</span> conda-forge uwsgi<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><h2 id="配置-uwsgi">配置 uWSGI</h2><p>在 <code>main.py</code> 同个目录创建一个 <code>uwsgi.ini</code> 文件:</p><figure><div class="code-wrapper"><pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token section"><span class="token punctuation">[</span><span class="token section-name selector">uwsgi</span><span class="token punctuation">]</span></span><span class="token key attr-name">module</span> <span class="token punctuation">=</span> <span class="token value attr-value">main:app</span><span class="token key attr-name">uid</span> <span class="token punctuation">=</span> <span class="token value attr-value">www-data</span><span class="token key attr-name">gid</span> <span class="token punctuation">=</span> <span class="token value attr-value">www-data</span><span class="token key attr-name">master</span> <span class="token punctuation">=</span> <span class="token value attr-value">true</span><span class="token key attr-name">processes</span> <span class="token punctuation">=</span> <span class="token value attr-value">5</span><span class="token key attr-name">socket</span> <span class="token punctuation">=</span> <span class="token value attr-value">/tmp/uwsgi.socket</span><span class="token key attr-name">chmod-sock</span> <span class="token punctuation">=</span> <span class="token value attr-value">664</span><span class="token key attr-name">vacuum</span> <span class="token punctuation">=</span> <span class="token value attr-value">true</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在这份配置文件中，调用的模块是 <code>main.py</code> 的 <code>app</code>。然后使用 <code>www-data</code>(WEB 服务的标准用户) 作为 <code>uwsgi</code> 进程的 <code>uid/gid</code>。 通过 <code>processes</code> 指定 5 个进程。另外，我们给 <code>uwsgi</code> 创建了一个 socket 文件 <code>/tmp/uwsgi.socket</code>，并赋予 664 的执行权限。 作为对比，也可以直接给 socket 设置端口号，例如：、</p><figure><div class="code-wrapper"><pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token section"><span class="token punctuation">[</span><span class="token section-name selector">uwsgi</span><span class="token punctuation">]</span></span>...<span class="token key attr-name">socket</span> <span class="token punctuation">=</span> <span class="token value attr-value">:3032</span>...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>另外，当我们退出进程时，希望 <code>/tmp/uwsgi.socket</code> 文件能够被自动删除，因此可以设置 <code>vacuum = true</code> 来实现这个功能。</p><p>运行 <code>uwsgi</code></p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">uwsgi uwsgi.ini<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>得到如下输出结果：</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">[uWSGI] getting INI configuration from uwsgi.ini*** Starting uWSGI 2.0.21 (64bit) on [Tue Apr 18 21:13:37 2023] ***compiled with version: 11.2.0 on 28 March 2023 07:32:14os: Linux-5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023nodename: Airmemachine: x86_64clock source: unixpcre jit disableddetected number of CPU cores: 20current working directory: &#x2F;home&#x2F;aizpy&#x2F;Workspace&#x2F;FlaskTestdetected binary path: &#x2F;home&#x2F;aizpy&#x2F;miniconda3&#x2F;envs&#x2F;flask&#x2F;bin&#x2F;uwsgiyour processes number limit is 63631your memory page size is 4096 bytesdetected max file descriptor number: 1024lock engine: pthread robust mutexesthunder lock: disabled (you can enable it with --thunder-lock)uwsgi socket 0 bound to UNIX address &#x2F;tmp&#x2F;uwsgi.socket fd 3Python version: 3.11.2 (main, Mar 27 2023, 23:42:44) [GCC 11.2.0]*** Python threads support is disabled. You can enable it with --enable-threads ***Python main interpreter initialized at 0xa01a98your server socket listen backlog is limited to 100 connectionsyour mercy for graceful operations on workers is 60 secondsmapped 437520 bytes (427 KB) for 5 cores*** Operational MODE: preforking ***WSGI app 0 (mountpoint&#x3D;&#39;&#39;) ready in 0 seconds on interpreter 0xa01a98 pid: 13221 (default app)*** uWSGI is running in multiple interpreter mode ***spawned uWSGI master process (pid: 13221)spawned uWSGI worker 1 (pid: 13222, cores: 1)spawned uWSGI worker 2 (pid: 13223, cores: 1)spawned uWSGI worker 3 (pid: 13224, cores: 1)spawned uWSGI worker 4 (pid: 13225, cores: 1)spawned uWSGI worker 5 (pid: 13226, cores: 1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>确实创建了 5 个进程。当输入<code>Ctrl-C</code> 来退出进程时，可以看到如下输出：</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">^CSIGINT&#x2F;SIGTERM received...killing workers...worker 1 buried after 1 secondsworker 2 buried after 1 secondsworker 3 buried after 1 secondsworker 4 buried after 1 secondsworker 5 buried after 1 secondsgoodbye to uWSGI.VACUUM: unix socket &#x2F;tmp&#x2F;uwsgi.socket removed.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>此时，5 个进程全部退出了，并且 <code>/tmp/uwsgi.socket</code> 文件也被删除了。</p><h2 id="生成依赖环境">生成依赖环境</h2><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip freeze <span class="token operator">></span> requirements.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>需要注意的是，如果你是通过 conda 安装的 uwsgi，那么执行这个命令后，uWSGI 这一条会变成类似这样的结果：</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">uWSGI @ file:&#x2F;&#x2F;&#x2F;croot&#x2F;uwsgi_1679988297904&#x2F;work<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>这是一个绝对路径，无法部署到其他机器上。此时可以根据之前 conda 的安装结果，来指定 uwsgi 的版本号。例如： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">uWSGI&#x3D;&#x3D;2.0.21<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure></p><p>最终的 <code>requirements.txt</code> 长这样:</p><figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">Flask&#x3D;&#x3D;2.2.3uWSGI&#x3D;&#x3D;2.0.21<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure><p>这里删掉了中间的依赖项，因为安装这两个库就会自动安装其他的。</p><h2 id="配置-nginx">配置 nginx</h2><p>创建一份 <code>nginx.conf</code> 文件：</p><figure><div class="code-wrapper"><pre class="line-numbers language-nginx" data-language="nginx"><code class="language-nginx"><span class="token directive"><span class="token keyword">user</span> www-data</span><span class="token punctuation">;</span><span class="token directive"><span class="token keyword">worker_processes</span> auto</span><span class="token punctuation">;</span><span class="token directive"><span class="token keyword">pid</span> /run/nginx.pid</span><span class="token punctuation">;</span><span class="token directive"><span class="token keyword">events</span></span> <span class="token punctuation">&#123;</span>    <span class="token directive"><span class="token keyword">worker_connections</span> <span class="token number">1024</span></span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">use</span> epoll</span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">multi_accept</span> <span class="token boolean">on</span></span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token directive"><span class="token keyword">http</span></span> <span class="token punctuation">&#123;</span>    <span class="token directive"><span class="token keyword">access_log</span> /dev/stdout</span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">error_log</span> /dev/stdout</span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">sendfile</span>            <span class="token boolean">on</span></span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">tcp_nopush</span>          <span class="token boolean">on</span></span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">tcp_nodelay</span>         <span class="token boolean">on</span></span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">keepalive_timeout</span>   <span class="token number">65</span></span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">types_hash_max_size</span> <span class="token number">2048</span></span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">include</span>             /etc/nginx/mime.types</span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">default_type</span>        application/octet-stream</span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">index</span>   index.html index.htm</span><span class="token punctuation">;</span>    <span class="token directive"><span class="token keyword">server</span></span> <span class="token punctuation">&#123;</span>        <span class="token directive"><span class="token keyword">listen</span>       <span class="token number">80</span> default_server</span><span class="token punctuation">;</span>        <span class="token directive"><span class="token keyword">listen</span>       [::]:80 default_server</span><span class="token punctuation">;</span>        <span class="token directive"><span class="token keyword">server_name</span>  localhost</span><span class="token punctuation">;</span>        <span class="token directive"><span class="token keyword">root</span>         /var/www/html</span><span class="token punctuation">;</span>        <span class="token directive"><span class="token keyword">location</span> /</span> <span class="token punctuation">&#123;</span>            <span class="token directive"><span class="token keyword">include</span> uwsgi_params</span><span class="token punctuation">;</span>            <span class="token directive"><span class="token keyword">uwsgi_pass</span> unix:/tmp/uwsgi.socket</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><h2 id="创建执行脚本">创建执行脚本</h2><p>创建一个启动脚本 <code>start.sh</code></p><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/usr/bin/env bash</span><span class="token function">service</span> nginx startuwsgi <span class="token parameter variable">--ini</span> uwsgi.ini<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure><h2 id="编写-dockerfile">编写 Dockerfile</h2><figure><div class="code-wrapper"><pre class="line-numbers language-docker" data-language="docker"><code class="language-docker"><span class="token instruction"><span class="token keyword">FROM</span> ubuntu:22.04</span><span class="token instruction"><span class="token keyword">MAINTAINER</span> aizpy <span class="token string">"aizpy@outlook.com"</span></span><span class="token instruction"><span class="token keyword">EXPOSE</span> 80</span><span class="token instruction"><span class="token keyword">COPY</span> . /srv/app</span><span class="token instruction"><span class="token keyword">WORKDIR</span> /srv/app</span><span class="token instruction"><span class="token keyword">RUN</span> apt-get clean &amp;&amp; apt-get -y update</span><span class="token instruction"><span class="token keyword">RUN</span> apt-get install -y nginx python3-dev build-essential pip</span><span class="token instruction"><span class="token keyword">RUN</span> pip install --no-cache-dir -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</span><span class="token instruction"><span class="token keyword">COPY</span> nginx.conf /etc/nginx</span><span class="token instruction"><span class="token keyword">RUN</span> chmod +x ./start.sh</span><span class="token instruction"><span class="token keyword">CMD</span> [<span class="token string">"./start.sh"</span>]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><h2 id="通过-dockerfile-创建镜像">通过 Dockerfile 创建镜像</h2><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> build <span class="token builtin class-name">.</span> <span class="token parameter variable">-t</span> my_flask_app<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><h2 id="生成容器并运行">生成容器并运行</h2><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> run <span class="token parameter variable">--name</span> flask_app <span class="token parameter variable">-p</span> <span class="token number">80</span>:80 my_flask_app<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><h2 id="参考">参考</h2><ul><li><a href="https://smirnov-am.github.io/running-flask-in-production-with-docker/">Running Flask in production with Docker</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>Flask</tag>
      
      <tag>uWSGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对 ChatGLM-6B 做 LoRA fine tuning</title>
    <link href="/2023/03/30/chatglm-6b-lora/"/>
    <url>/2023/03/30/chatglm-6b-lora/</url>
    
    <content type="html"><![CDATA[<p>ChatGLM-6B 是一个支持中英双语的对话语言模型，基于 GLM (General Language Model)。它只有 62 亿个参数，量化后最低 (INT4 量化) 只需要 6GB 的显存，完全可以部署到消费级显卡上。在实际使用这个模型一段时间以后，我们发现模型的对话表现能力确实非常不错。那么，基于这个模型做 Fine-tuning 就非常有价值了。</p><span id="more"></span><p><strong>声明：</strong></p><p>本文提供的所有技术信息，都基于 <a href="https://huggingface.co/THUDM/chatglm-6b">THUDM/chatglm-6b</a> 的历史版本： <code>096f3de6b4959ce38bef7bb05f3129c931a3084e</code>。</p><p>源码地址：</p><ul><li><a href="https://github.com/aizpy/chatglm-finetune">GitHub</a></li><li><a href="https://gitee.com/aizpy/chatglm-finetune">gitee</a></li></ul><h2 id="搭建依赖环境">搭建依赖环境</h2><p>安装 PyTorch 环境： <figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> torch torchvision torchaudio<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure></p><p>按照 ChatGLM-6B 的官方指导，安装软件依赖环境： <figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token assign-left variable">protobuf</span><span class="token operator">==</span><span class="token number">3.20</span>.0 <span class="token assign-left variable">transformers</span><span class="token operator">==</span><span class="token number">4.26</span>.1 icetk cpm_kernels  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure></p><p>为了做 LoRA，还要安装 peft <figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> peft<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure></p><h2 id="加载模型和-tokenizer">加载模型和 Tokenizer</h2><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelcheckpoint <span class="token operator">=</span> <span class="token string">"THUDM/chatglm-6b"</span>revision <span class="token operator">=</span> <span class="token string">"096f3de6b4959ce38bef7bb05f3129c931a3084e"</span>model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> revision<span class="token operator">=</span>revision<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> revision<span class="token operator">=</span>revision<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>正如前面声明所述，本文使用的历史版本号是 <code>096f3de6b4959ce38bef7bb05f3129c931a3084e</code>。如果开发者需要其他版本号，只需要更改 <code>revision</code> 值，并重新训练即可。</p><h2 id="分析模型结构">分析模型结构</h2><p>模型加载完后，我们可以打印这个 <code>model</code> 和 <code>tokenizer</code>，建立对模型的基本认知。</p><p>首先打印<code>model</code>： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure> 得到如下结果： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">ChatGLMForConditionalGeneration(  (transformer): ChatGLMModel(    (word_embeddings): Embedding(150528, 4096)    (layers): ModuleList(      (0-27): 28 x GLMBlock(        (input_layernorm): LayerNorm((4096,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)        (attention): SelfAttention(          (rotary_emb): RotaryEmbedding()          (query_key_value): Linear(in_features&#x3D;4096, out_features&#x3D;12288, bias&#x3D;True)          (dense): Linear(in_features&#x3D;4096, out_features&#x3D;4096, bias&#x3D;True)        )        (post_attention_layernorm): LayerNorm((4096,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)        (mlp): GLU(          (dense_h_to_4h): Linear(in_features&#x3D;4096, out_features&#x3D;16384, bias&#x3D;True)          (dense_4h_to_h): Linear(in_features&#x3D;16384, out_features&#x3D;4096, bias&#x3D;True)        )      )    )    (final_layernorm): LayerNorm((4096,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)  )  (lm_head): Linear(in_features&#x3D;4096, out_features&#x3D;150528, bias&#x3D;False))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 简单分析这个模型结构，至少可以得到如下一些信息：</p><ul><li>模型使用了 Transformer 结构，因此可以使用 LoRA 进行 Fine-tuning</li><li>从 Word Embedding 层可以看出，词汇表大小是 <code>150528</code></li><li>LoRA 可以操作的目标是：<code>query_key_value</code></li></ul><p>再打印<code>tokenizer</code>:</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>得到如下结果（为了便于阅读，已对结果做了分行处理）： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">ChatGLMTokenizer(name_or_path&#x3D;&#39;THUDM&#x2F;chatglm-6b&#39;, vocab_size&#x3D;150344, model_max_length&#x3D;2048, is_fast&#x3D;False, padding_side&#x3D;&#39;left&#39;, truncation_side&#x3D;&#39;right&#39;, special_tokens&#x3D;&#123;&#39;bos_token&#39;: &#39;&lt;sop&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;&#x2F;s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure></p><p>这里有几个可以关注的点：</p><ul><li>词汇表大小<code>vocab_size</code>是<code>150344</code></li><li>不是一个 fast Tokenizer（<code>is_fast</code> 的值是 <code>False</code>）</li><li>特殊 token 包括：<code>bos</code> <code>eos</code> <code>pad</code> 和 <code>mask</code></li></ul><p>为什么 model 中的词汇表大小是 <code>150528</code>，而 <code>tokenizer</code> 中定义的词汇表大小却是 <code>150344</code> 呢？读者可以带着这个疑问去读一读模型项目的源码，看看能不能找到答案。</p><h2 id="配置-lora">配置 LoRA</h2><p>借助 peft 库，我们可以很方便地对模型注入 LoRA。 <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> TaskType<span class="token keyword">def</span> <span class="token function">load_lora_config</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>    task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span>     inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>     lora_alpha<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>     lora_dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>    target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">return</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">)</span>model <span class="token operator">=</span> load_lora_config<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 打印可训练的参数量： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure> 得到如下结果： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">trainable params: 3670016 || all params: 6258876416 || trainable%: 0.05863697820615348<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure></p><p>可以看到，总的参数量是 <code>6,258,876,416</code>，可训练的参数量是 <code>3,670,016</code>，占比 <code>0.0586%</code> 左右。训练参数量只是百万级别的，可谓相当友好了！另外需要注意的一点是，ChatGLM-6B 是一个因果语言模型 (Causal Language Model)，因此我们这里选择的任务类型是 <code>CAUSAL_LM</code>。</p><h2 id="构建数据集">构建数据集</h2><h3 id="定义常量">定义常量</h3><p>构建之前，我们先定义几个特殊 Token 常量： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">bos <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>bos_token_ideop <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eop_token_idpad <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>pad_token_idmask <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>mask_token_idgmask <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>sp_tokenizer<span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>gMASK_token<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure></p><p>将这几个值打印出来： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"bos = "</span><span class="token punctuation">,</span> bos<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"eop = "</span><span class="token punctuation">,</span> eop<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"pad = "</span><span class="token punctuation">,</span> pad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"mask = "</span><span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"gmask = "</span><span class="token punctuation">,</span> gmask<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 得到如下结果： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">bos &#x3D;  150004eop &#x3D;  150005pad &#x3D;  20003mask &#x3D;  150000gmask &#x3D;  150001<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure></p><p>我们也可以直接用这个常量结果替换动态计算的部分。常量修改后的结果变成： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">bos <span class="token operator">=</span> <span class="token number">150004</span>eop <span class="token operator">=</span> <span class="token number">150005</span>pad <span class="token operator">=</span> <span class="token number">20003</span>mask <span class="token operator">=</span> <span class="token number">150000</span>gmask <span class="token operator">=</span> <span class="token number">150001</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 除了上面定义的 Token 常量，我们还需要定义模型训练绑定的设备名，以及最大输入长度和最大输出长度等，如下： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">device <span class="token operator">=</span> <span class="token string">"cuda"</span>max_src_length <span class="token operator">=</span> <span class="token number">200</span>max_dst_length <span class="token operator">=</span> <span class="token number">500</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure> 开发者可以结合自己的显卡性能和要处理的数据集特点来确定这些最大长度。</p><h3 id="测试-tokenizer-的编解码">测试 Tokenizer 的编解码</h3><p>我们可以先做个简单的测试： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">text <span class="token operator">=</span> <span class="token string">"AI探险家"</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>text<span class="token punctuation">,</span> add_special_tokens <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>text<span class="token punctuation">,</span> add_special_tokens <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure> 输出结果是： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">[26738, 98715, 83920, 150001, 150004][26738, 98715, 83920]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure></p><p>从这个结果可以看出，“<a href="https://aizpy.com/about">AI探险家</a>”这几个字的裸编码是 <code>[26738, 98715, 83920]</code>。为什么是这样呢？我们可以对每一个数值再解码，看看输出结果： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">26738</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">98715</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">83920</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure> 输出结果是： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">AI探险家<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure> 观察这个结果，读者应该能对词汇表建立基本的认知了。读者如果有兴趣，还可以分别针对 “A” “I” “探” “险” 这几个字分别编码，看看编码结果是什么。</p><p>另外，当 <code>add_special_tokens = True</code> 时，编码结果会在末尾添加 <code>150001</code>和 <code>150004</code>，也就是 <code>gmask</code> 和 <code>bos</code>。请注意，我们的训练数据，要按照如下编码要求进行构造： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">[token, ..., token, gmask, bos, token, ... token, eop]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure> 因此，前半部分文本的编码可以直接让 <code>add_special_tokens = True</code>，后半部分文本的编码则让 <code>add_special_tokens = False</code>，最后再拼接一个 <code>eop</code>。</p><h3 id="定义-prompt">定义 Prompt</h3><p>我们 Fine-tuning 的任务是问答任务（简称 QA），因此一个简单的 Prompt 是这样的：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">PROMPT_PATTERN <span class="token operator">=</span> <span class="token string">"问：&#123;&#125;\n答： "</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p><code>&#123;&#125;</code>里填入 QA 训练集的问题文本。在显存有限的情况下，如果不对长文本做限制处理，很容易出现类似 <code>CUDA out of memory</code> 这样的报错。处理长文本，在给定编码后的数组上限时，可能存在这么几种方式：</p><ul><li>截断<strong>末尾</strong>超出部分的编码</li><li>截断<strong>前面</strong>超出部分的编码</li><li>丢掉训练样本</li></ul><p>每一种方式都有各自的优劣，开发者可以根据自身数据的特点自行选择一种处理方式。当然，如果你的显存够大，也可以不处理。本文以上述第一种方式进行处理。 为了不把 <code>PROMPT_PATTERN</code> 中的 <code>\n答：</code> 这几个字截断掉，我们将整个 <code>PROMPT_PATTERN</code> 拆成两部分：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">PROMPT_PATTERN <span class="token operator">=</span> <span class="token string">"问：&#123;&#125;"</span>SEP_PATTERN <span class="token operator">=</span> <span class="token string">"\n答： "</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure><p>基于这份 Prompt 模板，我们定义下面三个辅助方法： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">create_prompt</span><span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> PROMPT_PATTERN<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">,</span> SEP_PATTERN<span class="token keyword">def</span> <span class="token function">create_prompt_ids</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> max_src_length<span class="token punctuation">)</span><span class="token punctuation">:</span>    prompt<span class="token punctuation">,</span> sep <span class="token operator">=</span> create_prompt<span class="token punctuation">(</span>question<span class="token punctuation">)</span>    sep_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>        sep<span class="token punctuation">,</span>         add_special_tokens <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token punctuation">)</span>    sep_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sep_ids<span class="token punctuation">)</span>    special_tokens_num <span class="token operator">=</span> <span class="token number">2</span>    prompt_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>        prompt<span class="token punctuation">,</span>         max_length <span class="token operator">=</span> max_src_length <span class="token operator">-</span> <span class="token punctuation">(</span>sep_len <span class="token operator">-</span> special_tokens_num<span class="token punctuation">)</span><span class="token punctuation">,</span>        truncation <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>        add_special_tokens <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> prompt_ids <span class="token operator">+</span> sep_ids<span class="token keyword">def</span> <span class="token function">create_inputs_and_labels</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> answer<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>    prompt <span class="token operator">=</span> create_prompt_ids<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> question<span class="token punctuation">,</span> max_src_length<span class="token punctuation">)</span>    completion <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>        answer<span class="token punctuation">,</span>         max_length <span class="token operator">=</span> max_dst_length<span class="token punctuation">,</span>        truncation <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>        add_special_tokens <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token punctuation">)</span>    inputs <span class="token operator">=</span> prompt <span class="token operator">+</span> completion <span class="token operator">+</span> <span class="token punctuation">[</span>eop<span class="token punctuation">]</span>    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span> <span class="token operator">+</span> completion <span class="token operator">+</span> <span class="token punctuation">[</span>eop<span class="token punctuation">]</span>         inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    <span class="token keyword">return</span> inputs<span class="token punctuation">,</span> labels<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 值得注意的两点：</p><ul><li>从 <code>create_prompt_ids</code> 这个函数实现可以看出，我们编码分隔符 <code>SEP_PATTERN</code> 时自动添加了前面所述的 2 个特殊 Token。</li><li>对 <code>create_inputs_and_labels</code> 的函数实现中，我们将 <code>labels</code> 无需处理的部分用数值 <code>-100</code> 来表示。因为 <code>ChatGLMForConditionalGeneration</code> 内部在计算损失函数的时候，用的是 <code>torch.nn.CrossEntropyLoss</code>。该函数的参数之一 <code>ignore_index</code> 默认值是 <code>-100</code>。这就让我们在计算损失函数时，无需考虑非标识部分的数值。</li></ul><h3 id="构建-attention-mask-和-position-ids">构建 Attention Mask 和 Position IDs</h3><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_attention_mask</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>    seq <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>    context_len <span class="token operator">=</span> seq<span class="token punctuation">.</span>index<span class="token punctuation">(</span>bos<span class="token punctuation">)</span>    seq_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span>    attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    attention_mask<span class="token punctuation">.</span>tril_<span class="token punctuation">(</span><span class="token punctuation">)</span>    attention_mask<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>context_len<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>    attention_mask<span class="token punctuation">.</span>unsqueeze_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    attention_mask <span class="token operator">=</span> <span class="token punctuation">(</span>attention_mask <span class="token operator">&lt;</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> attention_mask<span class="token keyword">def</span> <span class="token function">get_position_ids</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> device<span class="token punctuation">,</span> position_encoding_2d<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    seq <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>    context_len <span class="token operator">=</span> seq<span class="token punctuation">.</span>index<span class="token punctuation">(</span>bos<span class="token punctuation">)</span>    seq_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span>    mask_token <span class="token operator">=</span> mask <span class="token keyword">if</span> mask <span class="token keyword">in</span> seq <span class="token keyword">else</span> gmask    use_gmask <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token keyword">if</span> mask <span class="token keyword">in</span> seq <span class="token keyword">else</span> gmask    mask_position <span class="token operator">=</span> seq<span class="token punctuation">.</span>index<span class="token punctuation">(</span>mask_token<span class="token punctuation">)</span>    <span class="token keyword">if</span> position_encoding_2d<span class="token punctuation">:</span>        position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> use_gmask<span class="token punctuation">:</span>            position_ids<span class="token punctuation">[</span>context_len<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> mask_position        block_position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>context_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>            torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_len <span class="token operator">-</span> context_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>        <span class="token punctuation">)</span><span class="token punctuation">)</span>        position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>position_ids<span class="token punctuation">,</span> block_position_ids<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> use_gmask<span class="token punctuation">:</span>            position_ids<span class="token punctuation">[</span>context_len<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> mask_position        <span class="token keyword">return</span> position_ids<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在这个通用实现中，我们针对 <code>mask</code> 和 <code>gmask</code> 两种情况做了区分，同时也对是否执行 <code>position_encoding_2d</code> 分情况处理。本文的 QA 任务采用的是 <code>gmask</code>，并且使用 <code>position_encoding_2d = True</code>。</p><p>我们可以构建下面的问答，来验证下这几个函数的输出： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">test_data <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"question"</span><span class="token punctuation">:</span> <span class="token string">"AI探险家帅不帅？"</span><span class="token punctuation">,</span><span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"非常帅！"</span><span class="token punctuation">&#125;</span>inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> create_inputs_and_labels<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token operator">**</span>test_data<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>attention_mask <span class="token operator">=</span> get_attention_mask<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>position_ids <span class="token operator">=</span> get_position_ids<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"inputs: \n"</span><span class="token punctuation">,</span> inputs<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nlabels: \n"</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nposition_ids: \n"</span><span class="token punctuation">,</span> position_ids<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nattention_mask: \n"</span><span class="token punctuation">,</span> attention_mask<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure></p><p>输出结果（为了便于阅读，已对输出进行格式化操作）： <figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">inputs:  [20005, 84286, 20012, 31943, 98715, 83920, 87359, 83848, 87359, 20031, 20005, 20004, 87342, 20012, 150001, 150004, 20005, 84122, 87359, 20035, 150005]labels:  [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 20005, 84122, 87359, 20035, 150005]position_ids:  [ [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  1,  2,  3,  4,  5] ]attention_mask:  [[ [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True],  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 结合论文观察数据，基本符合预期。</p><h3 id="创建数据集">创建数据集</h3><p>我们先定义具有如下格式的训练数据： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">train_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">"question"</span><span class="token punctuation">:</span> <span class="token string">"问题1"</span><span class="token punctuation">,</span> <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"答案1"</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">&#123;</span><span class="token string">"question"</span><span class="token punctuation">:</span> <span class="token string">"问题2"</span><span class="token punctuation">,</span> <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"答案2"</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></div></figure> 定义好格式后，我们先创建一个 <code>QADataset</code> 类，如下： <figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token keyword">class</span> <span class="token class-name">QADataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">,</span> tokenizer<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> data        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> tokenizer     <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        item_data <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span>        tokenizer <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer        input_ids<span class="token punctuation">,</span> labels <span class="token operator">=</span> create_inputs_and_labels<span class="token punctuation">(</span>            tokenizer<span class="token punctuation">,</span>             device<span class="token operator">=</span>device<span class="token punctuation">,</span>            <span class="token operator">**</span>item_data        <span class="token punctuation">)</span>                attention_mask <span class="token operator">=</span> get_attention_mask<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> device<span class="token punctuation">)</span>        position_ids <span class="token operator">=</span> get_position_ids<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> device<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>            <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>            <span class="token string">"labels"</span><span class="token punctuation">:</span> labels<span class="token punctuation">,</span>            <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>            <span class="token string">"position_ids"</span><span class="token punctuation">:</span> position_ids        <span class="token punctuation">&#125;</span>            <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure></p><p>然后创建一个 Data Collator：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">collate_fn</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    attention_mask <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    position_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> obj <span class="token keyword">in</span> batch<span class="token punctuation">:</span>        input_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>obj<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>obj<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        attention_mask<span class="token punctuation">.</span>append<span class="token punctuation">(</span>obj<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        position_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>obj<span class="token punctuation">[</span><span class="token string">'position_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>        <span class="token string">'input_ids'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>attention_mask<span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token string">'labels'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token string">'position_ids'</span><span class="token punctuation">:</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>position_ids<span class="token punctuation">)</span>    <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><h2 id="开始训练">开始训练</h2><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments<span class="token punctuation">,</span> Trainermodel<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>    <span class="token string">"output"</span><span class="token punctuation">,</span>    fp16 <span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    save_steps <span class="token operator">=</span> <span class="token number">500</span><span class="token punctuation">,</span>    save_total_limit <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    per_device_train_batch_size <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>    learning_rate <span class="token operator">=</span> <span class="token number">1e-4</span><span class="token punctuation">,</span>    max_steps<span class="token operator">=</span><span class="token number">1500</span><span class="token punctuation">,</span>    logging_steps<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    remove_unused_columns<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    seed<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    data_seed<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    group_by_length<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    dataloader_pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">ModifiedTrainer</span><span class="token punctuation">(</span>Trainer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">compute_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> return_outputs<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> model<span class="token punctuation">(</span>            input_ids<span class="token operator">=</span>inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            attention_mask<span class="token operator">=</span>inputs<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            position_ids<span class="token operator">=</span>inputs<span class="token punctuation">[</span><span class="token string">"position_ids"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            labels<span class="token operator">=</span>inputs<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>losstrain_dataset <span class="token operator">=</span> QADataset<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>trainer <span class="token operator">=</span> ModifiedTrainer<span class="token punctuation">(</span>    model<span class="token operator">=</span>model<span class="token punctuation">,</span>    train_dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span>    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>    data_collator<span class="token operator">=</span>collate_fn<span class="token punctuation">,</span>    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><h2 id="预测">预测</h2><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"AI探险家的颜值如何？"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure><h2 id="保存训练模型">保存训练模型</h2><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">def</span> <span class="token function">save_tuned_parameters</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>    saved_params <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> v<span class="token punctuation">.</span>requires_grad    <span class="token punctuation">&#125;</span>    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>saved_params<span class="token punctuation">,</span> path<span class="token punctuation">)</span>save_tuned_parameters<span class="token punctuation">(</span>model<span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">"/path/to/output"</span><span class="token punctuation">,</span> <span class="token string">"chatglm-6b-lora.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><h2 id="重载训练后的模型">重载训练后的模型</h2><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">checkpoint <span class="token operator">=</span> <span class="token string">"THUDM/chatglm-6b"</span>revision <span class="token operator">=</span> <span class="token string">"096f3de6b4959ce38bef7bb05f3129c931a3084e"</span>model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> revision<span class="token operator">=</span>revision<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> revision<span class="token operator">=</span>revision<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> load_lora_config<span class="token punctuation">(</span>model<span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"/path/to/output/chatglm-6b-lora.pt"</span></span><span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>half<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"AI探险家的颜值如何？"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LoRA</tag>
      
      <tag>ChatGLM</tag>
      
      <tag>Transformer</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
